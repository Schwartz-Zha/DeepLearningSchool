{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "df2 = pd.read_csv('~/Documents/DeepLearningProjects/harvard_dl_winter2019/Labs/Lab3/ner_dataset.csv',encoding='latin1')\n",
    "#df2 = pd.read_csv('ner_dataset.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words :  35179  and number of tags :  17\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.fillna(method=\"ffill\")\n",
    "df2.tail(10)\n",
    "words = list(set(df2[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "tags = list(set(df2[\"Tag\"].values))\n",
    "n_words, n_tags = len(words), len(tags)\n",
    "print('Number of Words : ', len(words), ' and number of tags : ', len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(df2)\n",
    "print(getter.get_next())\n",
    "sentences = getter.sentences\n",
    "\n",
    "#max_len = 50\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "max_len = 50\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Differences I Made*\n",
    "The Bi-LSTM has 100 units and each gives an output value. But the label number is only 17 in total.\n",
    "I may think there should be a Dense layer to be inserted between the output and LSTM. ï¼ˆNot so reasonable, I just assume)\n",
    "But acctually the accuracy is not as high as the origial version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38846 samples, validate on 4317 samples\n",
      "Epoch 1/5\n",
      "38846/38846 [==============================] - 91s 2ms/step - loss: 1.1269 - acc: 0.9311 - val_loss: 0.3895 - val_acc: 0.9328\n",
      "Epoch 2/5\n",
      "38846/38846 [==============================] - 96s 2ms/step - loss: 0.2633 - acc: 0.9337 - val_loss: 0.1701 - val_acc: 0.9328\n",
      "Epoch 3/5\n",
      "38846/38846 [==============================] - 98s 3ms/step - loss: 0.1605 - acc: 0.9518 - val_loss: 0.1243 - val_acc: 0.9605\n",
      "Epoch 4/5\n",
      "38846/38846 [==============================] - 98s 3ms/step - loss: 0.1216 - acc: 0.9655 - val_loss: 0.0977 - val_acc: 0.9753\n",
      "Epoch 5/5\n",
      "38846/38846 [==============================] - 96s 2ms/step - loss: 0.0947 - acc: 0.9759 - val_loss: 0.0764 - val_acc: 0.9817\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=90, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "model = TimeDistributed(Dense(50,activation='softmax',use_bias=True))(model)\n",
    "model = Dropout(0.1)(model)\n",
    "out = (Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=5, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            (True ): Pred\n",
      "Initial        : (O    ): O\n",
      "results        : (O    ): O\n",
      "indicate       : (O    ): O\n",
      "the            : (O    ): O\n",
      "ruling         : (O    ): O\n",
      "party          : (O    ): O\n",
      "won            : (O    ): O\n",
      "a              : (O    ): O\n",
      "majority       : (O    ): O\n",
      ",              : (O    ): O\n",
      "but            : (O    ): O\n",
      "opposition     : (O    ): O\n",
      "groups         : (O    ): O\n",
      "say            : (O    ): O\n",
      "the            : (O    ): O\n",
      "vote           : (O    ): O\n",
      "was            : (O    ): O\n",
      "rigged         : (O    ): O\n",
      "and            : (O    ): O\n",
      "declared       : (O    ): O\n",
      "victory        : (O    ): O\n",
      ".              : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n",
      "ENDPAD         : (O    ): O\n"
     ]
    }
   ],
   "source": [
    "i = 2318\n",
    "p = model.predict(np.array([X_te[i]]))\n",
    "t = [tags[k] for k in np.argmax(y_te[i], axis=-1)]\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "for w, true, pred in zip(X_te[i], t, p[0]):\n",
    "    print(\"{:15}: ({:5}): {}\".format(words[w], true, tags[pred]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
